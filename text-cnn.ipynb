{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建计算图——LSTM模型\n",
    "#     embedding\n",
    "#     LSTM\n",
    "#     fc\n",
    "#     train_op\n",
    "# 训练流程代码\n",
    "# 数据级封装\n",
    "#     api: next_batch(batch_size)\n",
    "# 词表的封装：\n",
    "#     api: sentence2id(text_sentence): 句子转换成id\n",
    "# 类别的封装：\n",
    "#     api: category2id(text_category).\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO) # 打印日志的声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_embedding_size = 32, # 每一个词的embedding大小，即输入的通道数\n",
    "        num_timesteps = 600,  # 固定每一个输入的词语数量为50\n",
    "        num_filters = 128,\n",
    "        num_kernel_size = 3,\n",
    "        num_fc_nodes = 64, # 全连接层输出的维度\n",
    "        batch_size = 100,\n",
    "        learning_rate = 0.001, # 训练率\n",
    "        num_word_threshold = 10,  # 词语出现的最低频数，低于这个值的词语忽略\n",
    "    )\n",
    "\n",
    "hps = get_default_params()\n",
    "\n",
    "train_file = 'cnews_data/cnews.train.seg.txt'\n",
    "val_file = 'cnews_data/cnews.val.seg.txt'\n",
    "test_file = 'cnews_data/cnews.test.seg.txt'\n",
    "vocab_file = 'cnews_data/cnews.vocab.txt'\n",
    "category_file = 'cnews_data/cnews.category.txt'\n",
    "output_folder = 'cnews_data/run_text_run'\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77325\n",
      "INFO:tensorflow:num_classes: 10\n",
      "INFO:tensorflow:label: 时尚, id: 4\n"
     ]
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, filename, num_word_threshold): # threshold: 阈值\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._num_word_threshold = num_word_threshold\n",
    "        self._read_dict(filename)\n",
    "        \n",
    "    def _read_dict(self, filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, frequency = line.strip('\\r\\n').split('\\t')\n",
    "            frequency = int(frequency)\n",
    "            if frequency < self._num_word_threshold:\n",
    "                continue\n",
    "            idx = len(self._word_to_id)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            self._word_to_id[word] = idx\n",
    "    \n",
    "    def word_to_id(self, word):\n",
    "        # 第二个参数为若指定key不存在时的返回值\n",
    "        return self._word_to_id.get(word, self._unk) \n",
    "    \n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "    \n",
    "    def sentence_to_id(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split()]\n",
    "        return word_ids\n",
    "    \n",
    "    \n",
    "class CategoryDict:\n",
    "    def __init__(self, filename):\n",
    "        self._category_to_id = {}\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            category = line.strip('\\r\\n')\n",
    "            idx = len(self._category_to_id)\n",
    "            self._category_to_id[category] = idx\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self._category_to_id)\n",
    "    \n",
    "    def category_to_id(self, category):\n",
    "        if not category in self._category_to_id:\n",
    "            raise Exception(\n",
    "                '%s is not in our category lsit' % category)\n",
    "        return self._category_to_id[category]\n",
    "        \n",
    "vocab = Vocab(vocab_file, hps.num_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info('vocab_size: %d' % vocab_size)\n",
    "\n",
    "category_vocab = CategoryDict(category_file)\n",
    "num_classes = category_vocab.size()\n",
    "tf.logging.info('num_classes: %s' % num_classes)\n",
    "test_str = '时尚'\n",
    "tf.logging.info(\n",
    "    'label: %s, id: %d' % (\n",
    "        test_str,\n",
    "        category_vocab.category_to_id(test_str)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading data from cnews_data/cnews.train.seg.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/cnews.val.seg.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/cnews.test.seg.txt\n",
      "50000\n",
      "5000\n",
      "10000\n",
      "(array([[ 6751, 12824,   410, ...,    12,   428, 15385],\n",
      "       [35979,   325,  6792, ...,     0,     0,     0]], dtype=int32), array([8, 5], dtype=int32))\n",
      "(array([[ 2473,   225,    88, ...,     0,     0,     0],\n",
      "       [13208,    44,    11, ...,  1777,    22,     1]], dtype=int32), array([0, 8], dtype=int32))\n",
      "(array([[ 239, 2341, 1028, ...,    1,  129,   33],\n",
      "       [6987, 4035, 3629, ...,    1,  172,  836]], dtype=int32), array([9, 9], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "class TextDataSet:\n",
    "    def __init__(self, filename, vocab, category_vocab, num_timesteps):\n",
    "        self._vocab = vocab\n",
    "        self._category_vocab = category_vocab\n",
    "        self._num_timesteps = num_timesteps\n",
    "        # matrix\n",
    "        self._inputs = []\n",
    "        # vector\n",
    "        self._outputs = []\n",
    "        self._indicator = 0\n",
    "        self._parse_file(filename)\n",
    "        \n",
    "    def _parse_file(self, filename):\n",
    "        tf.logging.info('Loading data from %s' , filename)\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            label, content = line.strip('\\r\\n').split('\\t')\n",
    "            id_label = self._category_vocab.category_to_id(label)\n",
    "            id_words = self._vocab.sentence_to_id(content)\n",
    "            id_words = id_words[0:self._num_timesteps]\n",
    "            padding_num = self._num_timesteps - len(id_words)\n",
    "            id_words = id_words + [\n",
    "                self._vocab.unk for i in range(padding_num)]\n",
    "            self._inputs.append(id_words)\n",
    "            self._outputs.append(id_label)\n",
    "        self._inputs = np.asarray(self._inputs, dtype = np.int32)\n",
    "        self._outputs = np.asarray(self._outputs, dtype = np.int32)\n",
    "        self._random_shuffle()\n",
    "        self._num_examples = len(self._inputs)\n",
    "        \n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(len(self._inputs))\n",
    "        self._inputs = self._inputs[p]\n",
    "        self._outputs = self._outputs[p]\n",
    "        \n",
    "    def num_examples(self):\n",
    "        return self._num_examples\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "            end_indicator = batch_size\n",
    "        if end_indicator > len(self._inputs):\n",
    "            raise Exception('Batch_size : %d is too large' % batch_size)\n",
    "        batch_inputs = self._inputs[self._indicator:end_indicator]\n",
    "        batch_outputs = self._outputs[self._indicator:end_indicator]\n",
    "        self._indicator = end_indicator\n",
    "        return batch_inputs,batch_outputs\n",
    "    \n",
    "train_dataset = TextDataSet(\n",
    "    train_file, vocab, category_vocab, hps.num_timesteps)\n",
    "val_dataset = TextDataSet(\n",
    "    val_file, vocab, category_vocab, hps.num_timesteps)\n",
    "test_dataset = TextDataSet(\n",
    "    test_file, vocab, category_vocab, hps.num_timesteps)\n",
    "\n",
    "print(train_dataset.num_examples())\n",
    "print(val_dataset.num_examples())\n",
    "print(test_dataset.num_examples())\n",
    "\n",
    "print(train_dataset.next_batch(2))\n",
    "print(val_dataset.next_batch(2))\n",
    "print(test_dataset.next_batch(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64)\n",
      "(100, 64)\n",
      "(100, 10)\n",
      "softmax_loss:  (100,)\n"
     ]
    }
   ],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "    \n",
    "    # [100, 600]\n",
    "    inputs = tf.placeholder(tf.int32, [batch_size, num_timesteps])\n",
    "    outputs = tf.placeholder(tf.int32, [batch_size,])\n",
    "    # dropout 中的保留的神经元的数目\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    # 保存当前训练到的步数\n",
    "    global_step = tf.Variable(\n",
    "        tf.zeros([], tf.int64), name = 'global_step', trainable = False)\n",
    "    \n",
    "    # [100, 600, 32]\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope(\n",
    "        'embedding', initializer = embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embedding',\n",
    "            [vocab_size, hps.num_embedding_size],\n",
    "            tf.float32)\n",
    "        # [1, 10, 7] -> [embeddings[1], embeddings[10], embeddings[7]]\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "    \n",
    "    # [100, 600, 64]\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_size + hps.num_filters) / 3.0\n",
    "    cnn_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('cnn',initializer=cnn_init):\n",
    "        conv1 = tf.layers.conv1d(embed_inputs,\n",
    "                                 hps.num_filters,\n",
    "                                 hps.num_kernel_size,\n",
    "                                 activation = tf.nn.relu,\n",
    "                                 name = 'conv1')\n",
    "        global_maxpooling = tf.reduce_max(conv1, axis = 1)\n",
    "        \n",
    "        \n",
    "    fc_init = tf.initializers.variance_scaling(scale = 1.0, distribution = 'uniform')\n",
    "    with tf.variable_scope('fc', initializer = fc_init):\n",
    "        # [100, 64]\n",
    "        fc1 = tf.layers.dense(global_maxpooling,\n",
    "                              hps.num_fc_nodes,\n",
    "                              activation = tf.nn.relu,\n",
    "                              name = 'fc1')\n",
    "        print(fc1.shape)\n",
    "        # [100, 64]\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        print(fc1_dropout.shape)\n",
    "        # [100, 10]\n",
    "        logits = tf.layers.dense(fc1_dropout,\n",
    "                                 num_classes,\n",
    "                                 name = 'fc2')\n",
    "        print(logits.shape)\n",
    "        \n",
    "    with tf.name_scope('metrics'):\n",
    "        # [100,]\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits = logits, labels = outputs)\n",
    "        print(\"softmax_loss: \",softmax_loss.shape)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        # [0, 1, 5, 4, 2] -> argmax: 2\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits),\n",
    "                           1,\n",
    "                           output_type = tf.int32)\n",
    "        correct_pred = tf.equal(outputs, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "    with tf.name_scope('train_op'):\n",
    "        train_op = tf.train.AdamOptimizer(hps.learning_rate).minimize(\n",
    "            loss, global_step = global_step)\n",
    "        \n",
    "    return ((inputs, outputs, keep_prob),\n",
    "            (loss, accuracy),\n",
    "            (train_op, global_step))\n",
    "        \n",
    "placeholders, metrics, others = create_model(\n",
    "    hps, vocab_size, num_classes)\n",
    "\n",
    "inputs, outputs, keep_prob = placeholders\n",
    "loss, accuracy = metrics\n",
    "train_op, global_step = others\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   200, loss: 0.625, accuracy: 0.810\n",
      "INFO:tensorflow:Step:   400, loss: 0.258, accuracy: 0.940\n",
      "INFO:tensorflow:Step:   600, loss: 0.217, accuracy: 0.950\n",
      "INFO:tensorflow:Step:   800, loss: 0.260, accuracy: 0.930\n",
      "INFO:tensorflow:Step:  1000, loss: 0.124, accuracy: 0.970\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  1000, val_accuracy: 0.918, test_accuracy: 0.925\n",
      "INFO:tensorflow:Step:  1200, loss: 0.154, accuracy: 0.930\n",
      "INFO:tensorflow:Step:  1400, loss: 0.087, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  1600, loss: 0.139, accuracy: 0.950\n",
      "INFO:tensorflow:Step:  1800, loss: 0.066, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  2000, loss: 0.079, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  2000, val_accuracy: 0.914, test_accuracy: 0.938\n",
      "INFO:tensorflow:Step:  2200, loss: 0.046, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  2400, loss: 0.049, accuracy: 0.980\n",
      "INFO:tensorflow:Step:  2600, loss: 0.141, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  2800, loss: 0.041, accuracy: 0.970\n",
      "INFO:tensorflow:Step:  3000, loss: 0.046, accuracy: 0.980\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  3000, val_accuracy: 0.931, test_accuracy: 0.941\n",
      "INFO:tensorflow:Step:  3200, loss: 0.009, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  3400, loss: 0.015, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  3600, loss: 0.006, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  3800, loss: 0.008, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4000, loss: 0.005, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  4000, val_accuracy: 0.943, test_accuracy: 0.948\n",
      "INFO:tensorflow:Step:  4200, loss: 0.011, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4400, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4600, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  4800, loss: 0.059, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  5000, loss: 0.004, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  5000, val_accuracy: 0.944, test_accuracy: 0.948\n",
      "INFO:tensorflow:Step:  5200, loss: 0.009, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  5400, loss: 0.004, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  5600, loss: 0.009, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  5800, loss: 0.010, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  6000, loss: 0.019, accuracy: 0.990\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  6000, val_accuracy: 0.945, test_accuracy: 0.947\n",
      "INFO:tensorflow:Step:  6200, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  6400, loss: 0.003, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  6600, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  6800, loss: 0.000, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7000, loss: 0.000, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  7000, val_accuracy: 0.942, test_accuracy: 0.946\n",
      "INFO:tensorflow:Step:  7200, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7400, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7600, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  7800, loss: 0.003, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  8000, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  8000, val_accuracy: 0.945, test_accuracy: 0.946\n",
      "INFO:tensorflow:Step:  8200, loss: 0.002, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  8400, loss: 0.010, accuracy: 0.990\n",
      "INFO:tensorflow:Step:  8600, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  8800, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9000, loss: 0.003, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step:  9000, val_accuracy: 0.950, test_accuracy: 0.952\n",
      "INFO:tensorflow:Step:  9200, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9400, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9600, loss: 0.000, accuracy: 1.000\n",
      "INFO:tensorflow:Step:  9800, loss: 0.001, accuracy: 1.000\n",
      "INFO:tensorflow:Step: 10000, loss: 0.005, accuracy: 1.000\n",
      "INFO:tensorflow:Eval holdout: num_examples = 5000, batch_size = 100\n",
      "INFO:tensorflow:Eval holdout: num_examples = 10000, batch_size = 100\n",
      "INFO:tensorflow:Step: 10000, val_accuracy: 0.959, test_accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "def eval_holdout(sess, accuracy, dataset_for_test, batch_size):\n",
    "    num_batches = dataset_for_test.num_examples() // batch_size\n",
    "    tf.logging.info('Eval holdout: num_examples = %d, batch_size = %d',\n",
    "                    dataset_for_test.num_examples(), batch_size)\n",
    "    accuracy_vals = []\n",
    "    for i in range(num_batches):\n",
    "        batch_inputs, batch_labels = dataset_for_test.next_batch(batch_size)\n",
    "        accuracy_val = sess.run(accuracy,\n",
    "                                feed_dict = {\n",
    "                                    inputs: batch_inputs,\n",
    "                                    outputs: batch_labels,\n",
    "                                    keep_prob: 1.0,\n",
    "                                })\n",
    "        accuracy_vals.append(accuracy_val)\n",
    "    return np.mean(accuracy_vals)\n",
    "    \n",
    "init_op = tf.global_variables_initializer()\n",
    "train_keep_prob_value = 0.8\n",
    "\n",
    "num_train_steps = 10000\n",
    "\n",
    "# Train: 99.7%\n",
    "# Valid: 92.7%\n",
    "# Test: 93.2%\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    for i in range(num_train_steps):\n",
    "        batch_inputs, batch_labels = train_dataset.next_batch(hps.batch_size)\n",
    "        outputs_val = sess.run([loss, accuracy, train_op, global_step],\n",
    "                               feed_dict = {\n",
    "                                  inputs: batch_inputs,\n",
    "                                  outputs: batch_labels,\n",
    "                                  keep_prob: train_keep_prob_value,\n",
    "                               })\n",
    "        loss_val, accuracy_val, _, global_step_val = outputs_val\n",
    "        if global_step_val % 200 == 0:\n",
    "            tf.logging.info(\"Step: %5d, loss: %3.3f, accuracy: %3.3f\"\n",
    "                            % (global_step_val, loss_val, accuracy_val))\n",
    "        if global_step_val % 1000 == 0:\n",
    "            accuracy_eval = eval_holdout(\n",
    "                sess, accuracy, val_dataset, hps.batch_size)\n",
    "            accuracy_test = eval_holdout(\n",
    "                sess, accuracy, test_dataset, hps.batch_size)\n",
    "            tf.logging.info(\"Step: %5d, val_accuracy: %3.3f, test_accuracy: %3.3f\"\n",
    "                            % (global_step_val, accuracy_eval, accuracy_test))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
